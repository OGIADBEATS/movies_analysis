import os
import requests
from bs4 import BeautifulSoup
import time

base_url = "https://filmarks.com/list/year/2020s?page="
base_path = './movies_html'

#クローリング
def crawling():
    page_count = 1
    while True:
        Flag = False

        #Webページを取得
        response = requests.get(base_url + str(page_count))

        #HTMLをBeautiful Soupで解析
        soup = BeautifulSoup(response.text, 'html.parser')

        #最後のベージを過ぎたか判断(一部ページで「次へ」ボタンが表示されないためこのような処理にした)
        next_page_judje = soup.find('div', class_='p-timeline__zero')
        if next_page_judje and next_page_judje.get_text(strip=True) == '一致する情報は見つかりませんでした。':
            Flag = True

        if Flag == False:
            #取得したHTMLを保存
            file_name = os.path.join(base_path, 'output_page_{}.html'.format(page_count))
            with open(file_name, 'w', encoding='utf-8') as fw:
                fw.write(soup.prettify())

            print("Saved page{} to movies_html.".format(page_count))
            time.sleep(1)

            #次のページのURlを取得
            page_count += 1

        else:
          break

#main関数
def main():
    # HTMLを保存するディレクトリを作成
    if not os.path.exists(base_path):
        os.makedirs(base_path)

    crawling()
    print('Crawling Completed.')

if __name__ == "__main__":
    main()
